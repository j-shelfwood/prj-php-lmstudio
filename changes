diff --git a/CHANGELOG.md b/CHANGELOG.md
index 8e1d1e1..558cda3 100644
--- a/CHANGELOG.md
+++ b/CHANGELOG.md
@@ -14,6 +14,14 @@ and this project adheres to [Semantic Versioning](https://semver.org/spec/v2.0.0
 - Interactive `chat` command for chatting with language models
 - Added version number to composer.json
 - Added CHANGELOG.md file
+- Enhanced streaming support with retries, timeouts, and better error handling
+- Added `StreamingException` class for better error diagnostics
+- Added health check functionality to verify LMStudio server connection
+- Added new configuration options for connection timeouts and retries
+- Improved tool call handling with better JSON parsing
+- Added `DebugLogger` class for structured and configurable logging
+- Added support for file-based logging with configurable verbosity
+- Added PSR-3 logger compatibility for integration with existing logging systems
 
 ### Changed
 
@@ -21,11 +29,19 @@ and this project adheres to [Semantic Versioning](https://semver.org/spec/v2.0.0
 - Removed reflection usage in commands in favor of proper interfaces
 - Improved code organization and reduced duplication
 - Enhanced error handling in commands
+- Updated configuration file with new options for streaming and health checks
+- Replaced direct debug echo statements with structured logging
+- Improved streaming response handling with better error recovery
+- Enhanced tool call handling with incremental JSON parsing
 
 ### Fixed
 
 - Fixed type inconsistencies in client implementations
 - Improved error handling in streaming responses
+- Fixed JSON parsing issues in tool call arguments
+- Added retry mechanism for failed streaming requests
+- Fixed potential memory leaks in streaming response handling
+- Improved handling of idle connections during streaming
 
 ## [1.0.0] - 2025-02-22
 
diff --git a/README.md b/README.md
index 5b836f3..98eaf37 100644
--- a/README.md
+++ b/README.md
@@ -2,7 +2,7 @@
 
 A PHP package for integrating with LMStudio's local API. This library allows you to interact with LMStudio via both OpenAI‑compatible endpoints (/v1) and the new LM Studio REST API endpoints (/api/v0). It supports chat and text completions, embeddings, tool calls (including streaming responses), and is designed with dependency injection in mind.
 
-- PHP 8.2
+- PHP 8.2+
 - Version 1.1.0
 
 ## Features
@@ -36,10 +36,22 @@ A PHP package for integrating with LMStudio's local API. This library allows you
   - Artisan commands available for interactive usage (e.g., Chat, Models, Tools, ToolResponse).
 
 - **Command Line Interface:**
+
   - Interactive chat command for chatting with language models
   - Sequence command for testing all API endpoints
   - Configurable model and API selection
 
+- **Robust Streaming Support:**
+
+  - Automatic retries for failed streaming requests
+  - Configurable timeouts and connection settings
+  - Detailed error diagnostics with `StreamingException`
+
+- **Debugging & Logging:**
+  - Structured logging with configurable verbosity
+  - File-based logging support
+  - PSR-3 logger compatibility
+
 ## Installation
 
 You can install the package via composer:
@@ -50,6 +62,8 @@ composer require shelfwood/lmstudio-php
 
 ## Basic Usage
 
+### Simple Chat Completion
+
 ```php
 use Shelfwood\LMStudio\LMStudio;
 use Shelfwood\LMStudio\ValueObjects\ChatHistory;
@@ -72,6 +86,365 @@ $response = $lmstudio->openai()->chat($history->toArray(), [
 echo $response->choices[0]->message->content;
 ```
 
+### Text Completion
+
+```php
+use Shelfwood\LMStudio\LMStudio;
+
+// Create a new LMStudio instance
+$lmstudio = new LMStudio();
+
+// Get a text completion
+$response = $lmstudio->openai()->completion(
+    'Once upon a time in a land far, far away,',
+    ['model' => 'granite-3.1-8b-instruct']
+);
+
+echo $response->choices[0]->text;
+```
+
+### Embeddings
+
+```php
+use Shelfwood\LMStudio\LMStudio;
+
+// Create a new LMStudio instance
+$lmstudio = new LMStudio();
+
+// Get embeddings for a text
+$response = $lmstudio->openai()->embeddings(
+    'The quick brown fox jumps over the lazy dog',
+    ['model' => 'text-embedding-ada-002']
+);
+
+// Access the embedding vector
+$embedding = $response->data[0]->embedding;
+```
+
+## Advanced Usage
+
+### Using Request Objects
+
+For more control, you can use request objects:
+
+```php
+use Shelfwood\LMStudio\LMStudio;
+use Shelfwood\LMStudio\Requests\V1\ChatCompletionRequest;
+use Shelfwood\LMStudio\ValueObjects\ChatHistory;
+use Shelfwood\LMStudio\ValueObjects\Message;
+
+// Create a new LMStudio instance
+$lmstudio = new LMStudio();
+
+// Create a chat history
+$history = new ChatHistory([
+    Message::system('You are a helpful assistant.'),
+    Message::user('Write a short poem about programming.'),
+]);
+
+// Create a request object
+$request = new ChatCompletionRequest($history, 'granite-3.1-8b-instruct');
+$request = $request
+    ->withTemperature(0.7)
+    ->withMaxTokens(500)
+    ->withTopP(0.9);
+
+// Get a response
+$response = $lmstudio->openai()->chatCompletion($request);
+
+echo $response->choices[0]->message->content;
+```
+
+### Streaming Responses
+
+Streaming allows you to receive responses incrementally:
+
+```php
+use Shelfwood\LMStudio\LMStudio;
+use Shelfwood\LMStudio\Requests\V1\ChatCompletionRequest;
+use Shelfwood\LMStudio\ValueObjects\ChatHistory;
+use Shelfwood\LMStudio\ValueObjects\Message;
+
+// Create a new LMStudio instance
+$lmstudio = new LMStudio();
+
+// Create a chat history
+$history = new ChatHistory([
+    Message::system('You are a helpful assistant.'),
+    Message::user('Tell me a story about a robot.'),
+]);
+
+// Create a request object with streaming enabled
+$request = new ChatCompletionRequest($history, 'granite-3.1-8b-instruct');
+$request = $request->withStreaming(true);
+
+// Get a streaming response
+$stream = $lmstudio->openai()->streamChatCompletion($request);
+
+// Process the stream
+foreach ($stream as $chunk) {
+    if (isset($chunk['choices'][0]['delta']['content'])) {
+        echo $chunk['choices'][0]['delta']['content'];
+        flush(); // Flush output buffer to see results immediately
+    }
+}
+```
+
+### Accumulating Streaming Content
+
+You can also accumulate content from a streaming response:
+
+```php
+use Shelfwood\LMStudio\LMStudio;
+use Shelfwood\LMStudio\ValueObjects\ChatHistory;
+use Shelfwood\LMStudio\ValueObjects\Message;
+
+// Create a new LMStudio instance
+$lmstudio = new LMStudio();
+
+// Create a chat history
+$history = new ChatHistory([
+    Message::system('You are a helpful assistant.'),
+    Message::user('Explain quantum computing in simple terms.'),
+]);
+
+// Accumulate content from a streaming response
+$content = $lmstudio->openai()->accumulateChatContent($history, [
+    'model' => 'granite-3.1-8b-instruct',
+]);
+
+echo $content;
+```
+
+### Tool Functions
+
+Tool functions allow the model to call functions you define:
+
+```php
+use Shelfwood\LMStudio\LMStudio;
+use Shelfwood\LMStudio\Requests\V1\ChatCompletionRequest;
+use Shelfwood\LMStudio\ValueObjects\ChatHistory;
+use Shelfwood\LMStudio\ValueObjects\Message;
+use Shelfwood\LMStudio\ValueObjects\Tool;
+
+// Create a new LMStudio instance
+$lmstudio = new LMStudio();
+
+// Define a tool function
+$weatherTool = Tool::function(
+    'get_weather',
+    'Get the current weather in a given location',
+    [
+        'location' => [
+            'type' => 'string',
+            'description' => 'The city and state, e.g. San Francisco, CA',
+            'required' => true,
+        ],
+        'unit' => [
+            'type' => 'string',
+            'enum' => ['celsius', 'fahrenheit'],
+            'description' => 'The unit of temperature',
+            'required' => false,
+        ],
+    ]
+);
+
+// Create a chat history
+$history = new ChatHistory([
+    Message::system('You are a helpful assistant that can provide weather information.'),
+    Message::user('What\'s the weather like in San Francisco?'),
+]);
+
+// Create a request with tools
+$request = new ChatCompletionRequest($history, 'granite-3.1-8b-instruct');
+$request = $request
+    ->withTools([$weatherTool])
+    ->withToolChoice('auto');  // Let the model decide when to use tools
+
+// Get a response
+$response = $lmstudio->openai()->chatCompletion($request);
+
+// Check if the model used a tool
+$choice = $response->choices[0];
+if (isset($choice->message->toolCalls) && !empty($choice->message->toolCalls)) {
+    $toolCall = $choice->message->toolCalls[0];
+
+    // Get the function name and arguments
+    $functionName = $toolCall->function->name;
+    $arguments = json_decode($toolCall->function->arguments, true);
+
+    // Handle the tool call
+    if ($functionName === 'get_weather') {
+        $location = $arguments['location'] ?? 'unknown';
+        $unit = $arguments['unit'] ?? 'celsius';
+
+        // Call your actual weather API here
+        $weatherResponse = "It's 72°F (22°C) and sunny in {$location}.";
+
+        // Add the tool response to the chat history
+        $history->addMessage(Message::tool($weatherResponse, $toolCall->id));
+
+        // Get a new completion with the tool response
+        $request = new ChatCompletionRequest($history, 'granite-3.1-8b-instruct');
+        $response = $lmstudio->openai()->chatCompletion($request);
+
+        echo $response->choices[0]->message->content;
+    }
+}
+```
+
+### Custom Configuration
+
+You can customize the client configuration:
+
+```php
+use Shelfwood\LMStudio\Config\LMStudioConfig;
+use Shelfwood\LMStudio\LMStudio;
+
+// Create a custom configuration
+$config = new LMStudioConfig(
+    baseUrl: 'http://localhost:1234',
+    apiKey: 'your-api-key',
+    timeout: 60,
+    headers: ['X-Custom-Header' => 'Value'],
+    defaultModel: 'granite-3.1-8b-instruct',
+    connectTimeout: 15,
+    idleTimeout: 20,
+    maxRetries: 5,
+    healthCheckEnabled: true,
+    debugConfig: [
+        'enabled' => true,
+        'verbose' => true,
+        'log_file' => '/path/to/log/file.log',
+    ]
+);
+
+// Create a new LMStudio instance with the custom configuration
+$lmstudio = new LMStudio($config);
+
+// Or use the immutable configuration methods
+$lmstudio = (new LMStudio())
+    ->withBaseUrl('http://localhost:1234')
+    ->withApiKey('your-api-key')
+    ->withTimeout(60);
+```
+
+### Error Handling
+
+```php
+use Shelfwood\LMStudio\Exceptions\LMStudioException;
+use Shelfwood\LMStudio\Exceptions\StreamingException;
+use Shelfwood\LMStudio\LMStudio;
+use Shelfwood\LMStudio\ValueObjects\ChatHistory;
+use Shelfwood\LMStudio\ValueObjects\Message;
+
+// Create a new LMStudio instance
+$lmstudio = new LMStudio();
+
+// Create a chat history
+$history = new ChatHistory([
+    Message::system('You are a helpful assistant.'),
+    Message::user('Hello, how are you?'),
+]);
+
+try {
+    // Get a response
+    $response = $lmstudio->openai()->chat($history->toArray(), [
+        'model' => 'granite-3.1-8b-instruct',
+    ]);
+
+    echo $response->choices[0]->message->content;
+} catch (StreamingException $e) {
+    // Handle streaming-specific errors
+    echo "Streaming error: " . $e->getMessage();
+    echo "Chunks received: " . $e->getChunksReceived();
+    echo "Last chunk: " . $e->getLastChunk();
+    echo "Elapsed time: " . $e->getElapsedTime() . " seconds";
+} catch (LMStudioException $e) {
+    // Handle general errors
+    echo "Error: " . $e->getMessage();
+}
+```
+
+## Laravel Integration
+
+### Configuration
+
+After installing the package, publish the configuration file:
+
+```bash
+php artisan vendor:publish --tag=lmstudio-config
+```
+
+This will create a `config/lmstudio.php` file that you can customize.
+
+### Environment Variables
+
+Add these variables to your `.env` file:
+
+```
+LMSTUDIO_BASE_URL=http://localhost:1234
+LMSTUDIO_API_KEY=lm-studio
+LMSTUDIO_DEFAULT_MODEL=granite-3.1-8b-instruct
+LMSTUDIO_TIMEOUT=30
+LMSTUDIO_CONNECT_TIMEOUT=10
+LMSTUDIO_IDLE_TIMEOUT=15
+LMSTUDIO_MAX_RETRIES=3
+LMSTUDIO_HEALTH_CHECK_ENABLED=true
+LMSTUDIO_DEBUG=false
+LMSTUDIO_DEBUG_VERBOSE=false
+```
+
+### Using the Facade
+
+```php
+use Shelfwood\LMStudio\Facades\LMStudio;
+use Shelfwood\LMStudio\ValueObjects\ChatHistory;
+use Shelfwood\LMStudio\ValueObjects\Message;
+
+// Create a chat history
+$history = new ChatHistory([
+    Message::system('You are a helpful assistant.'),
+    Message::user('Hello, how are you?'),
+]);
+
+// Get a response using the facade
+$response = LMStudio::openai()->chat($history->toArray(), [
+    'model' => 'granite-3.1-8b-instruct',
+]);
+
+echo $response->choices[0]->message->content;
+```
+
+### Using Dependency Injection
+
+```php
+use Shelfwood\LMStudio\LMStudio;
+use Shelfwood\LMStudio\ValueObjects\ChatHistory;
+use Shelfwood\LMStudio\ValueObjects\Message;
+
+class MyController
+{
+    public function __invoke(LMStudio $lmstudio)
+    {
+        // Create a chat history
+        $history = new ChatHistory([
+            Message::system('You are a helpful assistant.'),
+            Message::user('Hello, how are you?'),
+        ]);
+
+        // Get a response
+        $response = $lmstudio->openai()->chat($history->toArray(), [
+            'model' => 'granite-3.1-8b-instruct',
+        ]);
+
+        return response()->json([
+            'response' => $response->choices[0]->message->content,
+        ]);
+    }
+}
+```
+
 ## Command Line Interface
 
 The package includes a command-line interface for interacting with LMStudio:
@@ -84,6 +457,63 @@ php bin/lmstudio chat
 php bin/lmstudio sequence
 ```
 
+### Chat Command Options
+
+```bash
+# Use a specific model
+php bin/lmstudio chat --model=granite-3.1-8b-instruct
+
+# Set a system message
+php bin/lmstudio chat --system="You are a helpful assistant that speaks like a pirate."
+
+# Set temperature
+php bin/lmstudio chat --temperature=0.8
+
+# Enable streaming (default)
+php bin/lmstudio chat --stream
+
+# Disable streaming
+php bin/lmstudio chat --no-stream
+```
+
+### Sequence Command Options
+
+```bash
+# Use a specific model
+php bin/lmstudio sequence --model=granite-3.1-8b-instruct
+
+# Show detailed output
+php bin/lmstudio sequence --verbose
+```
+
+## Debugging
+
+You can enable debugging to help troubleshoot issues:
+
+```php
+use Shelfwood\LMStudio\Config\LMStudioConfig;
+use Shelfwood\LMStudio\LMStudio;
+
+// Enable debugging
+$config = new LMStudioConfig(
+    debugConfig: [
+        'enabled' => true,
+        'verbose' => true,
+        'log_file' => '/path/to/log/file.log',
+    ]
+);
+
+$lmstudio = new LMStudio($config);
+```
+
+Or set environment variables:
+
+```
+LMSTUDIO_DEBUG=true
+LMSTUDIO_DEBUG_VERBOSE=true
+LMSTUDIO_DEBUG_LOG=/path/to/log/file.log
+```
+
 ## Changelog
 
 Please see [CHANGELOG](CHANGELOG.md) for more information on what has changed recently.
diff --git a/config/lmstudio.php b/config/lmstudio.php
index cb212b7..befb50e 100644
--- a/config/lmstudio.php
+++ b/config/lmstudio.php
@@ -34,4 +34,50 @@ return [
     */
 
     'default_model' => env('LMSTUDIO_DEFAULT_MODEL', 'granite-3.1-8b-instruct'),
+
+    /*
+    |--------------------------------------------------------------------------
+    | Connection Settings
+    |--------------------------------------------------------------------------
+    |
+    | Configure connection-specific settings like connect timeout and
+    | retry behavior for more reliable API interactions.
+    |
+    */
+
+    'connect_timeout' => env('LMSTUDIO_CONNECT_TIMEOUT', 10),
+
+    'idle_timeout' => env('LMSTUDIO_IDLE_TIMEOUT', 15),
+
+    'max_retries' => env('LMSTUDIO_MAX_RETRIES', 3),
+
+    /*
+    |--------------------------------------------------------------------------
+    | Health Check Configuration
+    |--------------------------------------------------------------------------
+    |
+    | Configure health check behavior to verify the LMStudio server
+    | is available before making requests.
+    |
+    */
+
+    'health_check' => [
+        'enabled' => env('LMSTUDIO_HEALTH_CHECK_ENABLED', true),
+        'interval' => env('LMSTUDIO_HEALTH_CHECK_INTERVAL', 5),
+    ],
+
+    /*
+    |--------------------------------------------------------------------------
+    | Debug Settings
+    |--------------------------------------------------------------------------
+    |
+    | Configure debug options for troubleshooting API interactions.
+    |
+    */
+
+    'debug' => [
+        'enabled' => env('LMSTUDIO_DEBUG', false),
+        'verbose' => env('LMSTUDIO_DEBUG_VERBOSE', false),
+        'log_file' => env('LMSTUDIO_DEBUG_LOG', storage_path('logs/lmstudio.log')),
+    ],
 ];
diff --git a/docs/tool-functions.md b/docs/tool-functions.md
deleted file mode 100644
index 8a31e47..0000000
--- a/docs/tool-functions.md
+++ /dev/null
@@ -1,147 +0,0 @@
-# Using Tool Functions
-
-Tool functions allow you to define custom functions that the model can call during a conversation. This is useful for tasks like retrieving information from external APIs, performing calculations, or taking actions based on user input.
-
-## Basic Usage
-
-Here's a simple example of how to use tool functions with the LMStudio PHP library:
-
-```php
-use Shelfwood\LMStudio\LMStudio;
-use Shelfwood\LMStudio\ValueObjects\ChatHistory;
-use Shelfwood\LMStudio\ValueObjects\Message;
-use Shelfwood\LMStudio\ValueObjects\Tool;
-use Shelfwood\LMStudio\Requests\V0\ChatCompletionRequest;
-
-// Create a new LMStudio instance
-$lmstudio = new LMStudio();
-
-// Define a tool function
-$weatherTool = Tool::function(
-    'get_weather',
-    'Get the current weather in a given location',
-    [
-        'location' => [
-            'type' => 'string',
-            'description' => 'The city and state, e.g. San Francisco, CA',
-            'required' => true,
-        ],
-        'unit' => [
-            'type' => 'string',
-            'enum' => ['celsius', 'fahrenheit'],
-            'description' => 'The unit of temperature',
-            'required' => false,
-        ],
-    ]
-);
-
-// Create a chat history
-$history = new ChatHistory([
-    Message::system('You are a helpful assistant that can provide weather information.'),
-    Message::user('What\'s the weather like in San Francisco?'),
-]);
-
-// Create a chat completion request with tools
-$request = new ChatCompletionRequest($history, 'gpt-3.5-turbo');
-$request = $request
-    ->withTools([$weatherTool])
-    ->withToolChoice('auto');  // Let the model decide when to use tools
-
-// Get a completion
-$response = $lmstudio->lms()->chatCompletion($request);
-```
-
-## Handling Tool Calls
-
-When the model decides to use a tool, you need to handle the tool call and provide a response:
-
-```php
-// Check if the model used a tool
-$choice = $response->choices[0];
-if (isset($choice->message->toolCalls) && !empty($choice->message->toolCalls)) {
-    $toolCall = $choice->message->toolCalls[0];
-
-    // Get the function name and arguments
-    $functionName = $toolCall->function->name;
-    $arguments = json_decode($toolCall->function->arguments, true);
-
-    // Handle the tool call based on the function name
-    if ($functionName === 'get_weather') {
-        $location = $arguments['location'] ?? 'unknown';
-        $unit = $arguments['unit'] ?? 'celsius';
-
-        // Call your actual weather API here
-        $weatherResponse = "It's 72°F (22°C) and sunny in {$location}.";
-
-        // Add the tool response to the chat history
-        $history->addMessage(Message::tool($weatherResponse, $toolCall->id));
-
-        // Create a new request with the updated history
-        $request = new ChatCompletionRequest($history, 'gpt-3.5-turbo');
-
-        // Get a new completion that incorporates the tool response
-        $response = $lmstudio->lms()->chatCompletion($request);
-    }
-}
-```
-
-## Tool Choice Options
-
-You can control when the model uses tools with the `withToolChoice` method:
-
-- `'auto'`: Let the model decide when to use tools
-- `'none'`: Disable tool usage
-- Specific function: Force the model to use a specific function
-
-```php
-// Let the model decide
-$request = $request->withToolChoice('auto');
-
-// Disable tool usage
-$request = $request->withToolChoice('none');
-
-// Force the model to use a specific function
-$request = $request->withToolChoice([
-    'type' => 'function',
-    'function' => ['name' => 'get_weather'],
-]);
-```
-
-## Streaming Tool Calls
-
-You can also use tool functions with streaming responses:
-
-```php
-// Enable streaming
-$request = $request->withStreaming(true);
-
-// Get a streaming completion
-$stream = $lmstudio->lms()->streamChatCompletion($request);
-
-// Process the stream
-foreach ($stream as $chunk) {
-    // Process each chunk
-}
-
-// Or accumulate tool calls from the stream
-$toolCalls = $lmstudio->lms()->accumulateChatToolCalls($history, [
-    'model' => 'gpt-3.5-turbo',
-    'tools' => [$weatherTool],
-]);
-```
-
-## Multiple Tools
-
-You can provide multiple tools to the model:
-
-```php
-$weatherTool = Tool::function(/* ... */);
-$calculatorTool = Tool::function(/* ... */);
-$searchTool = Tool::function(/* ... */);
-
-$request = $request->withTools([$weatherTool, $calculatorTool, $searchTool]);
-```
-
-## Complete Example
-
-For a complete example, see the [tool_functions.php](../examples/tool_functions.php) file in the examples directory.
diff --git a/src/Config/LMStudioConfig.php b/src/Config/LMStudioConfig.php
index 2671cb2..7347f75 100644
--- a/src/Config/LMStudioConfig.php
+++ b/src/Config/LMStudioConfig.php
@@ -6,20 +6,41 @@ namespace Shelfwood\LMStudio\Config;
 
 class LMStudioConfig
 {
+    /**
+     * @var array Debug configuration
+     */
+    private array $debugConfig;
+
     /**
      * @param  string  $baseUrl  The base URL for the LMStudio API
      * @param  string  $apiKey  The API key (not required for LMStudio but kept for OpenAI compatibility)
      * @param  int  $timeout  Request timeout in seconds
      * @param  array<string, string>  $headers  Additional headers to send with requests
      * @param  string|null  $defaultModel  The default model to use for requests
+     * @param  int|null  $connectTimeout  Connection timeout in seconds
+     * @param  int|null  $idleTimeout  Idle timeout for streaming in seconds
+     * @param  int|null  $maxRetries  Maximum number of retry attempts
+     * @param  bool|null  $healthCheckEnabled  Whether to perform health checks
+     * @param  array  $debugConfig  Debug configuration options
      */
     public function __construct(
         private string $baseUrl = 'http://localhost:1234',
         private string $apiKey = 'lm-studio',
         private int $timeout = 30,
         private array $headers = [],
-        private ?string $defaultModel = null
-    ) {}
+        private ?string $defaultModel = null,
+        private ?int $connectTimeout = 10,
+        private ?int $idleTimeout = 15,
+        private ?int $maxRetries = 3,
+        private ?bool $healthCheckEnabled = true,
+        array $debugConfig = []
+    ) {
+        $this->debugConfig = array_merge([
+            'enabled' => (bool) getenv('LMSTUDIO_DEBUG'),
+            'verbose' => (bool) getenv('LMSTUDIO_DEBUG_VERBOSE'),
+            'log_file' => getenv('LMSTUDIO_DEBUG_LOG') ?: null,
+        ], $debugConfig);
+    }
 
     /**
      * Get the base URL.
@@ -64,6 +85,48 @@ class LMStudioConfig
         ], $this->headers);
     }
 
+    /**
+     * Get the connection timeout.
+     */
+    public function getConnectTimeout(): ?int
+    {
+        return $this->connectTimeout;
+    }
+
+    /**
+     * Get the idle timeout for streaming.
+     */
+    public function getIdleTimeout(): ?int
+    {
+        return $this->idleTimeout;
+    }
+
+    /**
+     * Get the maximum number of retry attempts.
+     */
+    public function getMaxRetries(): ?int
+    {
+        return $this->maxRetries;
+    }
+
+    /**
+     * Check if health checks are enabled.
+     */
+    public function isHealthCheckEnabled(): ?bool
+    {
+        return $this->healthCheckEnabled;
+    }
+
+    /**
+     * Get the debug configuration.
+     *
+     * @return array The debug configuration
+     */
+    public function getDebugConfig(): array
+    {
+        return $this->debugConfig;
+    }
+
     /**
      * Create a new instance with a different base URL.
      */
@@ -120,4 +183,61 @@ class LMStudioConfig
 
         return $clone;
     }
+
+    /**
+     * Create a new instance with the given connection timeout.
+     */
+    public function withConnectTimeout(?int $connectTimeout): self
+    {
+        $clone = clone $this;
+        $clone->connectTimeout = $connectTimeout;
+
+        return $clone;
+    }
+
+    /**
+     * Create a new instance with the given idle timeout.
+     */
+    public function withIdleTimeout(?int $idleTimeout): self
+    {
+        $clone = clone $this;
+        $clone->idleTimeout = $idleTimeout;
+
+        return $clone;
+    }
+
+    /**
+     * Create a new instance with the given max retries.
+     */
+    public function withMaxRetries(?int $maxRetries): self
+    {
+        $clone = clone $this;
+        $clone->maxRetries = $maxRetries;
+
+        return $clone;
+    }
+
+    /**
+     * Create a new instance with health check enabled/disabled.
+     */
+    public function withHealthCheckEnabled(?bool $enabled): self
+    {
+        $clone = clone $this;
+        $clone->healthCheckEnabled = $enabled;
+
+        return $clone;
+    }
+
+    /**
+     * Create a new instance with the given debug configuration.
+     *
+     * @param  array  $debugConfig  The debug configuration
+     */
+    public function withDebugConfig(array $debugConfig): self
+    {
+        $clone = clone $this;
+        $clone->debugConfig = array_merge($this->debugConfig, $debugConfig);
+
+        return $clone;
+    }
 }
diff --git a/src/Exceptions/StreamingException.php b/src/Exceptions/StreamingException.php
new file mode 100644
index 0000000..75f8873
--- /dev/null
+++ b/src/Exceptions/StreamingException.php
@@ -0,0 +1,19 @@
+<?php
+
+declare(strict_types=1);
+
+namespace Shelfwood\LMStudio\Exceptions;
+
+class StreamingException extends LMStudioException
+{
+    public function __construct(
+        string $message,
+        int $code = 0,
+        ?\Throwable $previous = null,
+        public readonly ?string $lastChunk = null,
+        public readonly ?int $chunkCount = null,
+        public readonly ?float $elapsedTime = null
+    ) {
+        parent::__construct($message, $code, $previous);
+    }
+}
diff --git a/src/Http/Client.php b/src/Http/Client.php
index 4ee5b35..5122e3b 100644
--- a/src/Http/Client.php
+++ b/src/Http/Client.php
@@ -10,37 +10,51 @@ use GuzzleHttp\HandlerStack;
 use GuzzleHttp\Middleware;
 use Psr\Http\Message\RequestInterface;
 use Psr\Http\Message\ResponseInterface;
+use Psr\Log\LoggerInterface;
 use Shelfwood\LMStudio\Config\LMStudioConfig;
 use Shelfwood\LMStudio\Exceptions\LMStudioException;
+use Shelfwood\LMStudio\Exceptions\StreamingException;
 
 class Client
 {
     protected GuzzleClient $client;
 
-    protected bool $debug = false;
+    protected DebugLogger $logger;
 
     public function __construct(
-        private LMStudioConfig $config
+        private LMStudioConfig $config,
+        ?LoggerInterface $logger = null
     ) {
         // Create a handler stack with logging middleware if debug is enabled
         $stack = HandlerStack::create();
 
-        // Check if debug is enabled via environment variable
-        $this->debug = (bool) getenv('LMSTUDIO_DEBUG');
+        // Initialize the debug logger
+        $debugConfig = $config->getDebugConfig();
+        $this->logger = new DebugLogger(
+            $debugConfig['enabled'] ?? (bool) getenv('LMSTUDIO_DEBUG'),
+            $debugConfig['verbose'] ?? false,
+            $debugConfig['log_file'] ?? null,
+            $logger
+        );
 
-        if ($this->debug) {
+        if ($this->logger->isEnabled()) {
             // Add request logging
             $stack->push(Middleware::mapRequest(function (RequestInterface $request) {
-                echo "\n[DEBUG] Request: ".$request->getMethod().' '.$request->getUri()."\n";
-                echo '[DEBUG] Headers: '.json_encode($request->getHeaders())."\n";
+                $this->logger->logRequest(
+                    $request->getMethod(),
+                    (string) $request->getUri(),
+                    ['headers' => $request->getHeaders()]
+                );
 
                 return $request;
             }));
 
             // Add response logging
             $stack->push(Middleware::mapResponse(function (ResponseInterface $response) {
-                echo '[DEBUG] Response Status: '.$response->getStatusCode()."\n";
-                echo '[DEBUG] Response Headers: '.json_encode($response->getHeaders())."\n";
+                $this->logger->logResponse(
+                    $response->getStatusCode(),
+                    ['headers' => $response->getHeaders()]
+                );
 
                 return $response;
             }));
@@ -71,6 +85,16 @@ class Client
         return $this;
     }
 
+    /**
+     * Set the debug logger instance.
+     */
+    public function setLogger(DebugLogger $logger): self
+    {
+        $this->logger = $logger;
+
+        return $this;
+    }
+
     /**
      * Make a GET request.
      *
@@ -94,70 +118,149 @@ class Client
     }
 
     /**
-     * Make a streaming POST request.
+     * Send a streaming request to the API.
      *
-     * @throws LMStudioException
+     * @param  string  $uri  The URI to send the request to
+     * @param  array  $data  The data to send with the request
+     * @return \Generator A generator yielding decoded chunks
+     *
+     * @throws StreamingException If the request fails
      */
     public function stream(string $uri, array $data = []): \Generator
     {
-        try {
-            if ($this->debug) {
-                echo "\n[DEBUG] Streaming Request: POST ".$this->config->getBaseUrl().'/'.$uri."\n";
-                echo '[DEBUG] Streaming Data: '.json_encode($data)."\n";
-            }
-
-            $response = $this->client->post($uri, [
-                'json' => $data,
-                'stream' => true,
-            ]);
-
-            if ($this->debug) {
-                echo '[DEBUG] Streaming Response Status: '.$response->getStatusCode()."\n";
-            }
-
-            $buffer = '';
-            $stream = $response->getBody();
-
-            while (! $stream->eof()) {
-                $chunk = $stream->read(1024);
-                $buffer .= $chunk;
-
-                // Process complete SSE messages
-                while (($pos = strpos($buffer, "\n\n")) !== false) {
-                    $message = substr($buffer, 0, $pos);
-                    $buffer = substr($buffer, $pos + 2);
-
-                    foreach (explode("\n", $message) as $line) {
-                        if (str_starts_with($line, 'data: ')) {
-                            $data = substr($line, 6);
-
-                            if ($data === '[DONE]') {
-                                if ($this->debug) {
-                                    echo "[DEBUG] Streaming completed with [DONE] message\n";
+        $attempts = 0;
+        $maxAttempts = $this->config->getMaxRetries() ?? 3;
+        $backoffStrategy = [1, 2, 5]; // seconds
+        $startTime = microtime(true);
+        $chunkCount = 0;
+        $lastChunk = null;
+
+        while ($attempts < $maxAttempts) {
+            try {
+                $this->logger->logRequest('POST', $uri, $data);
+                $this->logger->log('Streaming request (attempt '.($attempts + 1).')', [
+                    'uri' => $uri,
+                    'max_attempts' => $maxAttempts,
+                ]);
+
+                $response = $this->client->post($uri, [
+                    'json' => $data,
+                    'stream' => true,
+                    'timeout' => $this->config->getTimeout(),
+                    'connect_timeout' => $this->config->getConnectTimeout() ?? 10,
+                ]);
+
+                $this->logger->logResponse($response->getStatusCode(), []);
+
+                $buffer = '';
+                $stream = $response->getBody();
+                $lastActivityTime = microtime(true);
+                $idleTimeout = $this->config->getIdleTimeout() ?? 15; // seconds
+
+                while (! $stream->eof()) {
+                    $chunk = $stream->read(1024);
+
+                    if (! empty($chunk)) {
+                        $lastActivityTime = microtime(true);
+                        $buffer .= $chunk;
+
+                        // Process complete SSE messages
+                        while (($pos = strpos($buffer, "\n\n")) !== false) {
+                            $message = substr($buffer, 0, $pos);
+                            $buffer = substr($buffer, $pos + 2);
+
+                            foreach (explode("\n", $message) as $line) {
+                                if (str_starts_with($line, 'data: ')) {
+                                    $data = substr($line, 6);
+
+                                    if ($data === '[DONE]') {
+                                        $duration = round(microtime(true) - $startTime, 2);
+                                        $this->logger->log('Streaming completed with [DONE]', [
+                                            'duration' => $duration,
+                                            'chunks' => $chunkCount,
+                                        ]);
+
+                                        return;
+                                    }
+
+                                    $decoded = json_decode($data, true);
+
+                                    if ($decoded !== null) {
+                                        $chunkCount++;
+                                        $lastChunk = $decoded;
+
+                                        // Log every 10th chunk to avoid excessive logging
+                                        if ($chunkCount % 10 === 0) {
+                                            $this->logger->log('Received chunks', [
+                                                'count' => $chunkCount,
+                                            ]);
+                                        }
+
+                                        $this->logger->logStreamingChunk($decoded, $chunkCount);
+
+                                        yield $decoded;
+                                    }
                                 }
-
-                                return;
                             }
+                        }
+                    } else {
+                        // Check for idle timeout
+                        if (microtime(true) - $lastActivityTime > $idleTimeout) {
+                            $this->logger->log('Stream idle timeout reached', [
+                                'idle_timeout' => $idleTimeout,
+                            ]);
+
+                            break;
+                        }
 
-                            $decoded = json_decode($data, true);
+                        // Small sleep to prevent CPU spinning
+                        usleep(50000); // 50ms
+                    }
 
-                            if ($decoded !== null) {
-                                if ($this->debug) {
-                                    echo '[DEBUG] Streaming chunk: '.json_encode($decoded)."\n";
-                                }
+                    // Check for overall timeout
+                    if (microtime(true) - $startTime > $this->config->getTimeout()) {
+                        $this->logger->log('Stream overall timeout reached', [
+                            'timeout' => $this->config->getTimeout(),
+                        ]);
 
-                                yield $decoded;
-                            }
-                        }
+                        break;
                     }
                 }
-            }
-        } catch (GuzzleException $e) {
-            if ($this->debug) {
-                echo '[DEBUG] Streaming error: '.$e->getMessage()."\n";
-            }
 
-            throw new LMStudioException($e->getMessage(), $e->getCode(), $e);
+                // If we got here without an exception, we're done
+                return;
+            } catch (\Exception $e) {
+                $attempts++;
+                $elapsedTime = microtime(true) - $startTime;
+
+                $this->logger->logError('Streaming error', $e);
+                $this->logger->log('Streaming error details', [
+                    'elapsed_time' => $elapsedTime,
+                    'chunks_received' => $chunkCount,
+                    'attempt' => $attempts,
+                ]);
+
+                if ($attempts >= $maxAttempts) {
+                    throw new StreamingException(
+                        "Streaming failed after {$attempts} attempts: ".$e->getMessage(),
+                        $e->getCode(),
+                        $e,
+                        $lastChunk ? json_encode($lastChunk) : null,
+                        $chunkCount,
+                        $elapsedTime
+                    );
+                }
+
+                $backoffTime = $backoffStrategy[min($attempts - 1, count($backoffStrategy) - 1)];
+
+                $this->logger->log('Retrying streaming request', [
+                    'backoff_time' => $backoffTime,
+                    'attempt' => $attempts,
+                    'max_attempts' => $maxAttempts,
+                ]);
+
+                sleep($backoffTime);
+            }
         }
     }
 
@@ -169,20 +272,49 @@ class Client
     private function request(string $method, string $uri, array $options = []): array
     {
         try {
+            $this->logger->logRequest($method, $uri, $options);
+
             $response = $this->client->request($method, $uri, $options);
             $contents = $response->getBody()->getContents();
 
-            if ($this->debug) {
-                echo '[DEBUG] Response Body: '.$contents."\n";
-            }
+            $this->logger->logResponse($response->getStatusCode(), $contents);
 
             return json_decode($contents, true);
         } catch (GuzzleException $e) {
-            if ($this->debug) {
-                echo '[DEBUG] Request error: '.$e->getMessage()."\n";
-            }
+            $this->logger->logError('Request error', $e);
 
             throw new LMStudioException($e->getMessage(), $e->getCode(), $e);
         }
     }
+
+    /**
+     * Check the health of the LMStudio server connection.
+     *
+     * @return bool True if the connection is healthy, false otherwise
+     */
+    public function checkHealth(): bool
+    {
+        try {
+            $this->logger->log('Performing health check', [
+                'base_url' => $this->config->getBaseUrl(),
+            ]);
+
+            // Try to get models as a simple health check
+            $response = $this->client->get('models', [
+                'timeout' => $this->config->getConnectTimeout() ?? 5,
+            ]);
+
+            $healthy = $response->getStatusCode() === 200;
+
+            $this->logger->log('Health check '.($healthy ? 'succeeded' : 'failed'), [
+                'status_code' => $response->getStatusCode(),
+            ]);
+
+            return $healthy;
+        } catch (\Exception $e) {
+            $this->logger->logError('Health check failed', $e);
+
+            return false;
+        }
+    }
 }
diff --git a/src/Http/DebugLogger.php b/src/Http/DebugLogger.php
new file mode 100644
index 0000000..edea07b
--- /dev/null
+++ b/src/Http/DebugLogger.php
@@ -0,0 +1,233 @@
+<?php
+
+declare(strict_types=1);
+
+namespace Shelfwood\LMStudio\Http;
+
+use Psr\Log\LoggerInterface;
+use Psr\Log\NullLogger;
+
+/**
+ * Debug logger for LMStudio API client.
+ */
+class DebugLogger
+{
+    /**
+     * @var LoggerInterface The PSR-3 logger instance
+     */
+    private LoggerInterface $logger;
+
+    /**
+     * @var bool Whether debug mode is enabled
+     */
+    private bool $enabled;
+
+    /**
+     * @var bool Whether verbose logging is enabled
+     */
+    private bool $verbose;
+
+    /**
+     * @var resource|null File handle for log file
+     */
+    private $fileHandle;
+
+    /**
+     * Create a new debug logger.
+     *
+     * @param  bool  $enabled  Whether debug mode is enabled
+     * @param  bool  $verbose  Whether verbose logging is enabled
+     * @param  string|null  $logFile  Path to log file (null for stdout)
+     * @param  LoggerInterface|null  $logger  PSR-3 logger instance (null for internal logger)
+     */
+    public function __construct(
+        bool $enabled = false,
+        bool $verbose = false,
+        ?string $logFile = null,
+        ?LoggerInterface $logger = null
+    ) {
+        $this->enabled = $enabled;
+        $this->verbose = $verbose;
+        $this->logger = $logger ?? new NullLogger;
+
+        if ($enabled && $logFile !== null) {
+            $this->fileHandle = @fopen($logFile, 'a');
+
+            if ($this->fileHandle === false) {
+                $this->fileHandle = null;
+                $this->log('Warning: Could not open log file: '.$logFile);
+            }
+        }
+    }
+
+    /**
+     * Log a debug message.
+     *
+     * @param  string  $message  The message to log
+     * @param  array  $context  Additional context data
+     */
+    public function log(string $message, array $context = []): void
+    {
+        if (! $this->enabled) {
+            return;
+        }
+
+        $timestamp = date('Y-m-d H:i:s');
+        $formattedMessage = "[{$timestamp}] {$message}";
+
+        if ($this->verbose && ! empty($context)) {
+            $formattedMessage .= "\nContext: ".json_encode($context, JSON_PRETTY_PRINT);
+        }
+
+        // Log to file if available
+        if ($this->fileHandle) {
+            fwrite($this->fileHandle, $formattedMessage.PHP_EOL);
+        } else {
+            // Log to stdout
+            echo $formattedMessage.PHP_EOL;
+        }
+
+        // Log to PSR-3 logger
+        $this->logger->debug($message, $context);
+    }
+
+    /**
+     * Log a request.
+     *
+     * @param  string  $method  The HTTP method
+     * @param  string  $uri  The request URI
+     * @param  array  $data  The request data
+     */
+    public function logRequest(string $method, string $uri, array $data = []): void
+    {
+        if (! $this->enabled) {
+            return;
+        }
+
+        $this->log("Request: {$method} {$uri}", [
+            'data' => $this->verbose ? $data : $this->summarizeData($data),
+        ]);
+    }
+
+    /**
+     * Log a response.
+     *
+     * @param  int  $statusCode  The HTTP status code
+     * @param  mixed  $body  The response body
+     */
+    public function logResponse(int $statusCode, $body): void
+    {
+        if (! $this->enabled) {
+            return;
+        }
+
+        $this->log("Response: Status {$statusCode}", [
+            'body' => $this->verbose ? $body : $this->summarizeData($body),
+        ]);
+    }
+
+    /**
+     * Log a streaming chunk.
+     *
+     * @param  mixed  $chunk  The streaming chunk
+     * @param  int  $chunkNumber  The chunk number
+     */
+    public function logStreamingChunk($chunk, int $chunkNumber): void
+    {
+        if (! $this->enabled || ! $this->verbose) {
+            return;
+        }
+
+        $this->log("Streaming chunk #{$chunkNumber}", [
+            'chunk' => $chunk,
+        ]);
+    }
+
+    /**
+     * Log an error.
+     *
+     * @param  string  $message  The error message
+     * @param  \Throwable|null  $exception  The exception that caused the error
+     */
+    public function logError(string $message, ?\Throwable $exception = null): void
+    {
+        if (! $this->enabled) {
+            return;
+        }
+
+        $context = [];
+
+        if ($exception) {
+            $context['exception'] = [
+                'class' => get_class($exception),
+                'message' => $exception->getMessage(),
+                'code' => $exception->getCode(),
+                'file' => $exception->getFile(),
+                'line' => $exception->getLine(),
+            ];
+
+            if ($this->verbose) {
+                $context['exception']['trace'] = $exception->getTraceAsString();
+            }
+        }
+
+        $this->log("ERROR: {$message}", $context);
+    }
+
+    /**
+     * Summarize data for non-verbose logging.
+     *
+     * @param  mixed  $data  The data to summarize
+     * @return mixed The summarized data
+     */
+    private function summarizeData($data)
+    {
+        if (is_array($data)) {
+            $result = [];
+
+            foreach ($data as $key => $value) {
+                if (is_array($value) && count($value) > 3) {
+                    $result[$key] = '[Array with '.count($value).' items]';
+                } elseif (is_string($value) && strlen($value) > 100) {
+                    $result[$key] = substr($value, 0, 100).'... [truncated]';
+                } else {
+                    $result[$key] = $value;
+                }
+            }
+
+            return $result;
+        }
+
+        if (is_string($data) && strlen($data) > 100) {
+            return substr($data, 0, 100).'... [truncated]';
+        }
+
+        return $data;
+    }
+
+    /**
+     * Check if debug mode is enabled.
+     */
+    public function isEnabled(): bool
+    {
+        return $this->enabled;
+    }
+
+    /**
+     * Check if verbose logging is enabled.
+     */
+    public function isVerbose(): bool
+    {
+        return $this->verbose;
+    }
+
+    /**
+     * Clean up resources.
+     */
+    public function __destruct()
+    {
+        if ($this->fileHandle) {
+            fclose($this->fileHandle);
+        }
+    }
+}
diff --git a/src/Http/StreamingResponseHandler.php b/src/Http/StreamingResponseHandler.php
index 9f5170a..429df69 100644
--- a/src/Http/StreamingResponseHandler.php
+++ b/src/Http/StreamingResponseHandler.php
@@ -41,7 +41,8 @@ class StreamingResponseHandler
         $currentToolCall = null;
         $currentId = null;
         $currentName = null;
-        $currentArguments = '';
+        $argumentsBuffer = '';
+        $inToolCall = false;
 
         foreach ($stream as $chunk) {
             if (! isset($chunk['choices'][0]['delta'])) {
@@ -55,7 +56,19 @@ class StreamingResponseHandler
                 foreach ($delta['tool_calls'] as $toolCallDelta) {
                     // New tool call with ID
                     if (isset($toolCallDelta['id'])) {
+                        // If we were building a previous tool call, try to parse its arguments
+                        if ($currentId !== null && ! empty($argumentsBuffer)) {
+                            try {
+                                $toolCalls[$currentId]['function']['arguments_parsed'] = json_decode($argumentsBuffer, true);
+                            } catch (\Exception $e) {
+                                // Keep raw arguments if parsing fails
+                                $toolCalls[$currentId]['function']['arguments_raw'] = $argumentsBuffer;
+                            }
+                        }
+
                         $currentId = $toolCallDelta['id'];
+                        $argumentsBuffer = '';
+                        $inToolCall = true;
                         $currentToolCall = [
                             'id' => $currentId,
                             'type' => $toolCallDelta['type'] ?? 'function',
@@ -69,19 +82,39 @@ class StreamingResponseHandler
 
                     // Update function name
                     if (isset($toolCallDelta['function']['name'])) {
-                        $currentName = $toolCallDelta['function']['name'];
-                        $toolCalls[$currentId]['function']['name'] = $currentName;
+                        $toolCalls[$currentId]['function']['name'] = $toolCallDelta['function']['name'];
                     }
 
                     // Append to arguments
                     if (isset($toolCallDelta['function']['arguments'])) {
-                        $currentArguments .= $toolCallDelta['function']['arguments'];
-                        $toolCalls[$currentId]['function']['arguments'] = $currentArguments;
+                        $argumentsBuffer .= $toolCallDelta['function']['arguments'];
+                        $toolCalls[$currentId]['function']['arguments'] = $argumentsBuffer;
+
+                        // Try to parse JSON incrementally
+                        try {
+                            $parsed = json_decode($argumentsBuffer, true);
+
+                            if (json_last_error() === JSON_ERROR_NONE) {
+                                $toolCalls[$currentId]['function']['arguments_parsed'] = $parsed;
+                            }
+                        } catch (\Exception $e) {
+                            // Ignore parsing errors during streaming
+                        }
                     }
                 }
             }
         }
 
+        // Final attempt to parse any remaining arguments
+        if ($currentId !== null && ! empty($argumentsBuffer)) {
+            try {
+                $toolCalls[$currentId]['function']['arguments_parsed'] = json_decode($argumentsBuffer, true);
+            } catch (\Exception $e) {
+                // Keep raw arguments if parsing fails
+                $toolCalls[$currentId]['function']['arguments_raw'] = $argumentsBuffer;
+            }
+        }
+
         return array_values($toolCalls);
     }
 
diff --git a/src/LMS.php b/src/LMS.php
index 3f2a7ba..52b686e 100644
--- a/src/LMS.php
+++ b/src/LMS.php
@@ -6,6 +6,7 @@ namespace Shelfwood\LMStudio;
 
 use Generator;
 use Shelfwood\LMStudio\Config\LMStudioConfig;
+use Shelfwood\LMStudio\Contracts\ConfigAwareInterface;
 use Shelfwood\LMStudio\Contracts\LMStudioClientInterface;
 use Shelfwood\LMStudio\Http\Client;
 use Shelfwood\LMStudio\Http\StreamingResponseHandler;
@@ -19,7 +20,7 @@ use Shelfwood\LMStudio\Responses\V0\TextCompletion;
 use Shelfwood\LMStudio\Traits\HandlesStreamingResponses;
 use Shelfwood\LMStudio\ValueObjects\ChatHistory;
 
-class LMS implements LMStudioClientInterface
+class LMS implements ConfigAwareInterface, LMStudioClientInterface
 {
     use HandlesStreamingResponses;
 
diff --git a/src/LMStudio.php b/src/LMStudio.php
index 7b91110..808ff3b 100644
--- a/src/LMStudio.php
+++ b/src/LMStudio.php
@@ -5,9 +5,10 @@ declare(strict_types=1);
 namespace Shelfwood\LMStudio;
 
 use Shelfwood\LMStudio\Config\LMStudioConfig;
+use Shelfwood\LMStudio\Contracts\ConfigAwareInterface;
 use Shelfwood\LMStudio\Contracts\LMStudioClientInterface;
 
-class LMStudio
+class LMStudio implements ConfigAwareInterface
 {
     private LMStudioConfig $config;
 
@@ -20,6 +21,14 @@ class LMStudio
         $this->config = $config ?? new LMStudioConfig;
     }
 
+    /**
+     * Get the client configuration.
+     */
+    public function getConfig(): LMStudioConfig
+    {
+        return $this->config;
+    }
+
     /**
      * Get the native LMStudio API client (v0).
      */
diff --git a/src/OpenAI.php b/src/OpenAI.php
index 13f86bf..5c8bf6d 100644
--- a/src/OpenAI.php
+++ b/src/OpenAI.php
@@ -6,6 +6,7 @@ namespace Shelfwood\LMStudio;
 
 use Generator;
 use Shelfwood\LMStudio\Config\LMStudioConfig;
+use Shelfwood\LMStudio\Contracts\ConfigAwareInterface;
 use Shelfwood\LMStudio\Contracts\LMStudioClientInterface;
 use Shelfwood\LMStudio\Http\Client;
 use Shelfwood\LMStudio\Http\StreamingResponseHandler;
@@ -19,7 +20,7 @@ use Shelfwood\LMStudio\Responses\V1\TextCompletion;
 use Shelfwood\LMStudio\Traits\HandlesStreamingResponses;
 use Shelfwood\LMStudio\ValueObjects\ChatHistory;
 
-class OpenAI implements LMStudioClientInterface
+class OpenAI implements ConfigAwareInterface, LMStudioClientInterface
 {
     use HandlesStreamingResponses;
 
diff --git a/src/Providers/LMStudioServiceProvider.php b/src/Providers/LMStudioServiceProvider.php
index b271b03..dc3b374 100644
--- a/src/Providers/LMStudioServiceProvider.php
+++ b/src/Providers/LMStudioServiceProvider.php
@@ -23,6 +23,11 @@ class LMStudioServiceProvider extends ServiceProvider
                 timeout: $config['timeout'] ?? 30,
                 headers: $config['headers'] ?? [],
                 defaultModel: $config['default_model'] ?? null,
+                connectTimeout: $config['connect_timeout'] ?? 10,
+                idleTimeout: $config['idle_timeout'] ?? 15,
+                maxRetries: $config['max_retries'] ?? 3,
+                healthCheckEnabled: $config['health_check']['enabled'] ?? true,
+                debugConfig: $config['debug'] ?? []
             );
         });
 
diff --git a/src/Traits/HandlesStreamingResponses.php b/src/Traits/HandlesStreamingResponses.php
index 966bf22..155bd0c 100644
--- a/src/Traits/HandlesStreamingResponses.php
+++ b/src/Traits/HandlesStreamingResponses.php
@@ -5,72 +5,92 @@ declare(strict_types=1);
 namespace Shelfwood\LMStudio\Traits;
 
 use Generator;
+use Shelfwood\LMStudio\Exceptions\StreamingException;
+use Shelfwood\LMStudio\Http\StreamingResponseHandler;
 
 /**
  * Trait for handling streaming responses from LMStudio API.
  */
 trait HandlesStreamingResponses
 {
+    /**
+     * Get the streaming response handler instance.
+     */
+    protected function getStreamingResponseHandler(): StreamingResponseHandler
+    {
+        return new StreamingResponseHandler;
+    }
+
     /**
      * Accumulate content from a streaming response.
      *
      * @param  \Generator  $stream  The streaming response
+     * @return string The accumulated content
+     *
+     * @throws StreamingException If the streaming response fails
      */
     protected function accumulateContent(\Generator $stream): string
     {
-        $content = '';
-
-        foreach ($stream as $chunk) {
-            if (isset($chunk['choices'][0]['delta']['content'])) {
-                $content .= $chunk['choices'][0]['delta']['content'];
-            } elseif (isset($chunk['choices'][0]['text'])) {
-                $content .= $chunk['choices'][0]['text'];
+        try {
+            return $this->getStreamingResponseHandler()->accumulateContent($stream);
+        } catch (\Exception $e) {
+            if ($e instanceof StreamingException) {
+                throw $e;
             }
-        }
 
-        return $content;
+            throw new StreamingException(
+                "Failed to accumulate content from streaming response: {$e->getMessage()}",
+                previous: $e
+            );
+        }
     }
 
     /**
      * Accumulate tool calls from a streaming response.
      *
      * @param  \Generator  $stream  The streaming response
+     * @return array The accumulated tool calls
+     *
+     * @throws StreamingException If the streaming response fails
      */
     protected function accumulateToolCalls(\Generator $stream): array
     {
-        $toolCalls = [];
-        $currentToolCall = null;
-
-        foreach ($stream as $chunk) {
-            if (isset($chunk['choices'][0]['delta']['tool_calls'])) {
-                $delta = $chunk['choices'][0]['delta']['tool_calls'][0];
-
-                // Initialize a new tool call if we have an ID
-                if (isset($delta['index']) && ! isset($toolCalls[$delta['index']])) {
-                    $toolCalls[$delta['index']] = [
-                        'id' => $delta['id'] ?? '',
-                        'type' => $delta['type'] ?? 'function',
-                        'function' => [
-                            'name' => '',
-                            'arguments' => '',
-                        ],
-                    ];
-                    $currentToolCall = $delta['index'];
-                }
+        try {
+            return $this->getStreamingResponseHandler()->accumulateToolCalls($stream);
+        } catch (\Exception $e) {
+            if ($e instanceof StreamingException) {
+                throw $e;
+            }
 
-                // Update the function name if present
-                if (isset($delta['function']['name']) && $currentToolCall !== null) {
-                    $toolCalls[$currentToolCall]['function']['name'] = $delta['function']['name'];
-                }
+            throw new StreamingException(
+                "Failed to accumulate tool calls from streaming response: {$e->getMessage()}",
+                previous: $e
+            );
+        }
+    }
 
-                // Append to the arguments if present
-                if (isset($delta['function']['arguments']) && $currentToolCall !== null) {
-                    $toolCalls[$currentToolCall]['function']['arguments'] .= $delta['function']['arguments'];
-                }
+    /**
+     * Process a streaming response with a callback for each chunk.
+     *
+     * @param  \Generator  $stream  The streaming response
+     * @param  callable  $callback  The callback to process each chunk
+     *
+     * @throws StreamingException If the streaming response fails
+     */
+    protected function processStreamWithCallback(\Generator $stream, callable $callback): void
+    {
+        try {
+            $this->getStreamingResponseHandler()->handle($stream, $callback);
+        } catch (\Exception $e) {
+            if ($e instanceof StreamingException) {
+                throw $e;
             }
-        }
 
-        return array_values($toolCalls);
+            throw new StreamingException(
+                "Failed to process streaming response: {$e->getMessage()}",
+                previous: $e
+            );
+        }
     }
 
     /**
@@ -78,9 +98,17 @@ trait HandlesStreamingResponses
      *
      * @param  array  $messages  The messages to generate a completion for
      * @param  array  $options  Additional options for the completion
+     * @return Generator The streaming response
      */
     public function streamChat(array $messages, array $options = []): Generator
     {
+        // Perform health check if enabled
+        if (isset($this->config) && $this->config->isHealthCheckEnabled()) {
+            if (! $this->client->checkHealth()) {
+                throw new StreamingException('LMStudio server is not available');
+            }
+        }
+
         return $this->client->stream('chat/completions', array_merge([
             'messages' => $messages,
             'stream' => true,
@@ -92,9 +120,17 @@ trait HandlesStreamingResponses
      *
      * @param  string  $prompt  The prompt to generate a completion for
      * @param  array  $options  Additional options for the completion
+     * @return Generator The streaming response
      */
     public function streamCompletion(string $prompt, array $options = []): Generator
     {
+        // Perform health check if enabled
+        if (isset($this->config) && $this->config->isHealthCheckEnabled()) {
+            if (! $this->client->checkHealth()) {
+                throw new StreamingException('LMStudio server is not available');
+            }
+        }
+
         return $this->client->stream('completions', array_merge([
             'prompt' => $prompt,
             'stream' => true,
